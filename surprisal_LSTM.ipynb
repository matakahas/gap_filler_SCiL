{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Is a pre-trained LSTM model aware of relative clause island constraints?\n",
        "\n",
        "Part of the code for tokenizing sentences and calculating token-by-token surprisal was adapted from https://github.com/kuribayashi4/surprisal_reading_time_en_ja, and edited in order to ensure that the code fits the purpose of this project and it runs without an issue in my environment."
      ],
      "metadata": {
        "id": "BkW6G0nYEq_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load repository for calculating surprisals"
      ],
      "metadata": {
        "id": "DyLk9mFfwlid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kuribayashi4/surprisal_reading_time_en_ja.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yvvdbpq9HDys",
        "outputId": "09027cb1-c26f-45f8-e10e-40988fa4ecc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'surprisal_reading_time_en_ja'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 41 (delta 10), reused 26 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (41/41), 3.17 MiB | 4.89 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/surprisal_reading_time_en_ja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQuYstexHKCV",
        "outputId": "e1841997-f3ff-4570-fd7c-55a155c59eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/surprisal_reading_time_en_ja\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#before executing this code, I changed the package name of \"mecab\" to \"mecab-python3\"\n",
        "!pip install -r requirements.txt\n",
        "!pip install unidic-lite"
      ],
      "metadata": {
        "id": "C3eQmlPGHNGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#access files saved in Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfy6eTTEHaWd",
        "outputId": "63294829-424f-46a0-874a-88d396bc2eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary packages\n",
        "import MeCab\n",
        "import unidic\n",
        "import unicodedata\n",
        "import mojimoji\n",
        "import torch\n",
        "import sentencepiece as spm\n",
        "import matplotlib.ticker as plticker\n",
        "import pandas as pd\n",
        "import japanize_matplotlib\n",
        "import numpy as np\n",
        "\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from fairseq.models.transformer_lm import TransformerLanguageModel\n",
        "from fairseq.models.lstm_lm import LSTMLanguageModel\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.pyplot import figure"
      ],
      "metadata": {
        "id": "sOpWd259H5zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the model"
      ],
      "metadata": {
        "id": "LYuTRXDbG7Ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "JA_MODEL_PATH = '/content/drive/MyDrive/Classes/LIGN 167 - FA22'\n",
        "\n",
        "pretrained_lm = LSTMLanguageModel.from_pretrained(JA_MODEL_PATH,\n",
        "                                                  'checkpoint_last_pretrained.pt',\n",
        "                                                  data_name_or_path='japanese-dict',\n",
        "                                                  bpe='sentencepiece',\n",
        "                                                  sentencepiece_model='japanese-dict/spm/japanese_gpt2_unidic.model')"
      ],
      "metadata": {
        "id": "p1-dDlZmJHzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load word divider and SentencePiece tokenizer\n",
        "wakati = MeCab.Tagger(\"-Owakati\")\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"japanese-dict/spm/japanese_gpt2_unidic.model\")"
      ],
      "metadata": {
        "id": "tazjS-gxLILt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2a73e0f-a633-4457-b2d6-357a3a0fc6c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_bos(tensor, bos):\n",
        "    return torch.cat([torch.tensor([bos]), tensor])\n",
        "\n",
        "loss_fct = CrossEntropyLoss(ignore_index=-1, reduce=False)\n",
        "\n",
        "def batch_surprisal(df, lm):\n",
        "  dataset = pd.DataFrame(columns = [\"island\", \"sent_number\", \"token\", \"surprisal\"])\n",
        "  sent_number = 0\n",
        "  for i, row in df.iterrows():\n",
        "    sent_number += 1\n",
        "    tokens = []\n",
        "    sent = unicodedata.normalize('NFKC', mojimoji.han_to_zen(row['sentence']))\n",
        "    sent_wakati = wakati.parse(sent).strip()\n",
        "    pieces = ' '.join(sp.EncodeAsPieces(sent_wakati))\n",
        "    input_ids = lm.binarize(pieces)\n",
        "    bos = lm.src_dict.bos()\n",
        "    input_ids_with_special_token = concat_bos(input_ids, bos)\n",
        "\n",
        "    results = lm.models[0](input_ids_with_special_token.view(1,-1))\n",
        "    surprisals = loss_fct(results[0][0][:-1], input_ids)\n",
        "    surprisals = surprisals.data.tolist()\n",
        "    assert len(surprisals) == len(input_ids)\n",
        "\n",
        "    for idx in input_ids:\n",
        "      tokens.append(lm.src_dict[idx].strip('‚ñÅ'))\n",
        "\n",
        "    if row['island'] == 1:\n",
        "      to_append = {'island': list(np.repeat(1, len(tokens), axis=0)),\n",
        "                   'sent_number': list(np.repeat(sent_number, len(tokens), axis=0)),\n",
        "                   'token': tokens,\n",
        "                   'surprisal': surprisals}\n",
        "      dataset = dataset.append(pd.DataFrame(to_append))\n",
        "    elif row['island'] == 0:\n",
        "      to_append = {'island': list(np.repeat(0, len(tokens), axis=0)),\n",
        "                   'sent_number': list(np.repeat(sent_number, len(tokens), axis=0)),\n",
        "                   'token': tokens,\n",
        "                   'surprisal': surprisals}\n",
        "      dataset = dataset.append(pd.DataFrame(to_append))\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "qRM_aKXULf6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8efea8e4-2d35-43d5-da10-bc941d67e495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import test data, and compute token-by-token surprisal using the language model"
      ],
      "metadata": {
        "id": "N_ishzbpHJfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/[file path]/test_stimuli.csv')  # change path names before running"
      ],
      "metadata": {
        "id": "kp0dDbIyLpvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get and save the result\n",
        "result = batch_surprisal(data, pretrained_lm)\n",
        "result.to_csv('result_LSTM.csv')"
      ],
      "metadata": {
        "id": "SYM8hvwJQVu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After exporting the result, I manually coded the region of interest (i.e., all the words following the noun that underwent long-distance relativization), and reimported the edited version.\n",
        "\n",
        "Example:\n",
        "\n",
        "Because the book that __ wrote was featured in the news, <ins>the professor looks proud.</ins>\n",
        "\n",
        "\\[The book that __ wrote was featured in the news\\] <ins>the professor looks proud.</ins>\n"
      ],
      "metadata": {
        "id": "NpY7as32wvlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_edited = pd.read_csv('/content/drive/MyDrive/[file path]/result_LSTM.csv') # change path names before running"
      ],
      "metadata": {
        "id": "R4urq6vFUc5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cond_list = []\n",
        "for i, row in result_edited.iterrows():\n",
        "  if row['sent_number'] < 9:\n",
        "    cond_list.append('noext')\n",
        "  elif row['sent_number'] > 16:\n",
        "    cond_list.append('ext_isl')\n",
        "  else:\n",
        "    cond_list.append('ext_noisl')\n",
        "\n",
        "result_edited['condition'] = cond_list"
      ],
      "metadata": {
        "id": "EFKXnQhkbLqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get mean surprisal on the critical region by group (island=1/non-island=0)\n",
        "critical = result_edited[result_edited['critical']==1]\n",
        "critical.groupby('condition')['surprisal'].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKDWZf2KUoG3",
        "outputId": "63f097a5-a74c-47fc-f540-4a2a7a0001ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "condition\n",
              "ext_isl      4.817421\n",
              "ext_noisl    4.859113\n",
              "noext        4.627623\n",
              "Name: surprisal, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results suggest that the pre-trained LSTM LM is no more surprised to see the sign of long-distance extraction with an island violation than the one without an island violation. In other words, the LM doesn't seem to be aware of the relative clause island constraint."
      ],
      "metadata": {
        "id": "qhYvHiwdJ4Sz"
      }
    }
  ]
}